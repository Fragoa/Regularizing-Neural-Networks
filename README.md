# Regularizing Neural Networks on MNIST

This project investigates the use of **L1**, **L2** regularization and **Dropout** to reduce overfitting in neural networks trained on the MNIST dataset.

## Overview

The model experiments were designed in several stages using both a **tiny subset** (5%) and the **full MNIST dataset**. The goal was to find configurations that maximize generalization without overfitting.

---

## ğŸ”¬ Experiment Stages

### 1. Baseline Model (M1)
- Architecture: Two hidden layers (8 and 16 neurons)
- Dataset: Tiny set & Full set
- Result: Slight overfitting on the tiny set; decent performance on the full set.

---

### 2. L1 Regularization (M1-L1)
- Tested Î» values from `1e-6` to `1e-1`.
- Tiny Set Best Î»: `1e-3`
- Full Set Best Î»: `1e-5`
- Conclusion: L1 helped reduce overfitting; careful tuning of Î» is essential.

---

### 3. L2 Regularization (M1-L2)
- Same Î» search procedure.
- L2 provided stable performance with slightly better accuracy in some cases.
- Conclusion: Comparable to L1, but slightly more robust.

---

### 4. Adding Dropout (Model M2)
- Based on best L1 model (Î» = `1e-5`)
- Dropout added after each hidden layer
- Tested 22 values for dropout rate `p` from `0.01` to `0.99`.

ğŸ“Œ **Best p (Dropout Rate):** `0.01`  
This setup provided the best balance between validation accuracy and loss, without overfitting.

---

## âœ… Final Results

| Model     | Regularization | Î»     | Dropout `p` | Overfitting | Accuracy (Test) |
|-----------|----------------|--------|--------------|--------------|------------------|
| M1        | None           | â€“      | â€“            | High         | Moderate         |
| M1-L1     | L1             | 1e-5   | â€“            | Reduced      | High             |
| M1-L2     | L2             | 1e-5   | â€“            | Reduced      | High             |
| **M2**    | L1 + Dropout   | 1e-5   | **0.01**     | Low          | **Highest**      |

---

## ğŸ“Œ Conclusion

- **Best Regularization Setup**: L1 with Î» = `1e-5`
- **Best Dropout Rate**: `p = 0.01`
- **Best Overall Model**: M2 â€” L1 + Dropout  
This combination achieved the highest test accuracy with minimal overfitting.

---

## ğŸ“ Files & Structure

- `model_training.py` â€” Contains training loops and architecture definitions.
- `dropout_experiments.py` â€” Code for evaluating dropout performance.
- `plots/` â€” Accuracy and loss graphs across epochs for all models.
- `results.csv` â€” Collected metrics for all configurations.

---

## ğŸ”§ Dependencies

- TensorFlow / Keras
- NumPy
- Matplotlib
- scikit-learn

Install dependencies:
```bash
pip install tensorflow numpy matplotlib scikit-learn
